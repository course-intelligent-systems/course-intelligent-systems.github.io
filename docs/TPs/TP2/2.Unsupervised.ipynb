{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised hands-on\n",
    "\n",
    "Today we are going to make sense of a classic dataset: [MNIST](https://en.wikipedia.org/wiki/MNIST_database) and we are going to continue using the [sklearn](https://scikit-learn.org/stable/) lib in Python.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn\n",
    "from sklearn import datasets, svm, metrics,cluster\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Numpy\n",
    "import numpy\n",
    "from numpy.random import normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell, and read carefully the code: it contains useful variables that you can re-use in this hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# We load the MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "image_shape = digits.images[0].shape\n",
    "\n",
    "# Take a look at the data\n",
    "_, axes = plt.subplots(nrows=1, ncols=20, figsize=(20, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "    ax.set_title(\"Label: %i\" % label)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples contains the number of images in the dataset\n",
    "n_samples =#TO COMPLETE (1 expression)\n",
    "\n",
    "# flatten the images as vectors\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the centroids of your [k-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.k_means.html) clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "k =#TO COMPLETE (1 expression)\n",
    "centroids,clu,variance =#TO COMPLETE (1 expression)\n",
    "\n",
    "\n",
    "# Plot the centroids\n",
    "_, axes = plt.subplots(nrows=1, ncols=k, figsize=(20, 3))\n",
    "for ax, image in zip(axes, centroids):\n",
    "    ax.set_axis_off()\n",
    "    to_plot = image.reshape(image_shape) # shape it back to an image\n",
    "    ax.imshow(to_plot, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the number of clusters and:\n",
    "\n",
    "Compute quality indicators (labels needed):\n",
    " - AMI\n",
    " - Confusion matrix with the real labels\n",
    " \n",
    "Real unsupervised measures:\n",
    " - Silhouette index\n",
    " - Intra-inter variance ratio (aka Calinski-Harabasz Index)\n",
    " \n",
    " \n",
    " You may look at the doc [here](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation).\n",
    " \n",
    " For some of the performance metrics, make a plot to decide on what is the best number of clusters. You can use the following snippet:\n",
    "`\n",
    "plt.clf()\n",
    "plt.plot(number_of_clust_list,performance_of_clustering_list)\n",
    "plt.show()\n",
    "`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score, confusion_matrix, silhouette_score, calinski_harabasz_score\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "Plot the points in a 2-coordinate plan using [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). Each point is a handwritten digit.\n",
    "You can make a scatter plot using the following snippet:\n",
    "`\n",
    "plt.clf()\n",
    "plt.scatter(data_to_plot[:,0],data_to_plot[:,1],c=labels)\n",
    "plt.show()\n",
    "`\n",
    "\n",
    "This will color the points according to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =#TO COMPLETE (1 expression)\n",
    "lower_dim_data =#TO COMPLETE (1 expression)\n",
    "colors = list(range(10))\n",
    "plt.clf()\n",
    "plt.scatter(lower_dim_data[:,0],lower_dim_data[:,1],c=digits.target) # we color by label\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many principal components are necessary to describe well the data?\n",
    "To measure this, you can check the amount of variance captured in the low dimensional space: if the variance (inter-point square of distances) is the same as in higher dimensional space, it means that you have enough components.\n",
    "    \n",
    "Make a plot (x-axis = number of components, y-axis = captured variance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =#TO COMPLETE (1 expression)\n",
 "#TO COMPLETE\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(list(range(60)),numpy.cumsum(pca.explained_variance_ratio_))\n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) for dimension reduction. Plot digits as a scatter plot (1 point == 1 instance of handwritten digit).\n",
    "\n",
    "Vary the perplexity parameter between 0.001 and 100 to make a lot of sense!\n",
    "\n",
    "Does it help?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities =#TO COMPLETE (1 expression)\n",
    "for perp in perplexities:\n",
    "    tsne =#TO COMPLETE (1 expression)\n",
    "    lower_dim_data =#TO COMPLETE (1 expression)\n",
    "    plt.clf()\n",
    "    plt.scatter(lower_dim_data[:,0],lower_dim_data[:,1],c=digits.target) # we color by label\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hybrid solution\n",
    "\n",
    "## Important exercise to understand one of the weird behaviors in high dimension\n",
    "\n",
    "Clustering directly in high dimension is not always a good idea. Let's prove it!\n",
    "\n",
    "\n",
    "Let's imagine that  $X_i$ is an image belonging to class $i$.\n",
    "$X_i$ is centered around an average image $\\mu_i$ and follows a multi variate covariance matrix $\\sigma^2.I_n$.\n",
    "\n",
    "Compute the expectation value of the distance between $X_i$ and $X_j$ with respect to $\\mu_i$, $\\mu_j$ and remarking that $\\sum\\limits_{k=1}^n Z_k^2 \\sim \\chi^2_n$ when $Z_k$ are independent and follow $\\mathcal{N}(0,1)$.\n",
    "\n",
    "\n",
    "What can you expect as the dimension grows? Can we hope to separate well classes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your Latex proof here:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's noise our data and see how the clustering works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_data = data + 13*normal(size=data.shape) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot tdata samples\n",
    "_, axes = plt.subplots(nrows=1, ncols=k, figsize=(20, 3))\n",
    "for ax, image in zip(axes, noisy_data):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image.reshape(image_shape), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "k=15 # you can let 10 here, but it turns out the effect is clearer with more clusters\n",
    "centroids,clu,variance =#TO COMPLETE (1 expression)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the centroids\n",
    "_, axes = plt.subplots(nrows=1, ncols=k, figsize=(20, 3))\n",
    "for ax, image in zip(axes, centroids):\n",
    "    ax.set_axis_off()\n",
 "#TO COMPLETE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you recognize the digits in the data samples?\n",
    "Answer:\n",
    "Can you recognize the digits in the centroids?\n",
    "Answer:\n",
    "\n",
    "Reduce the dimension using PCA (vary the number of components between 2 and 10), and then run a k-means. Vizualise the centroids back in full dimension using the `inverse_transform` function that maps back a point from low dimension to the high dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_choice =#TO COMPLETE (1 expression)\n",
    "k=15\n",
    "\n",
    "for n in comp_choice:\n",
    "    pca =#TO COMPLETE (1 expression)\n",
    "    lower_dim_data =#TO COMPLETE (1 expression)\n",
    "    # k-means on lower-dim data\n",
    "    centroids,clu,variance =#TO COMPLETE (1 expression)\n",
    "    # Plot the centroids\n",
    "    _, axes = plt.subplots(nrows=1, ncols=k, figsize=(20, 3))\n",
    "    for ax, centroid in zip(axes, centroids):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(pca.inverse_transform(centroid).reshape(image_shape), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Are the centroids of better quality (can we hint the number associated with a cluster)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
